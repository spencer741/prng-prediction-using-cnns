{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal Team 7\n",
    "\n",
    "Spencer Arnold <br/>\n",
    "Ryan Hines <br/>\n",
    "Matthew Hawks <br/>\n",
    "Jacob Anderson <br/>\n",
    "Tae Kweon <br/>\n",
    "Ali Najib <br/>\n",
    "\n",
    "Note: The design of the GAN is subject to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: block; float:left;\">\n",
    "    <h3>Code Def</h3>\n",
    "    \n",
    "| Acronyms | Definition                      |\n",
    "|----------|:-------------------------------:|\n",
    "| GAN      | Generative Adversarial Network  |\n",
    "| PRN      | Pseudo random number            |\n",
    "| PRNG     | Pseudo random number generator  |\n",
    "\n",
    
  
    "### Our Aim\n",
    "* To ***improve*** upon different methods of PRN generation using a modified GAN architecture, and in doing so,\n",
    "compare the \"cryptographic robustness\" of each method.</br>\n",
    "* Our secondary aim is to continue the quest [[1]](https://arxiv.org/ftp/arxiv/papers/1801/1801.01117.pdf)[[2]](https://arxiv.org/pdf/1810.00378.pdf) for insights on pseudo randomness.\n",
    "\n",
    "### The Meat\n",
    "Our proposal is to create a modified GAN, in which the generative network trains on a specific PRNG's output to be able to produce statistically similar data (n bits). The predictive network will then train on n-1 of the bits to predict the nth bit. The backpropagation will flow through the predictive network in addition to the generative network to acheive the adversarial nature of a traditional GAN setup, as described so eloquently below.\n",
    "\n",
    "> The predictor maximizes [gradient ascent] the probability of correctly predicting the nth value from the other [n-1] values, while the generator minimizes it [gradient descent]. Thus the pseudo-randomness of the generatorâ€™s output is formulated as unpredictability by an improving opponent. [reference](https://arxiv.org/pdf/1810.00378.pdf)\n",
    "\n",
    "This will be slightly different from a classic GAN setup, in which a discriminator determines classification of a \"true\" and \"fake\" distribution. With the a classic GAN (generative-discriminative) approach, we would likely end up merely modeling the chosen PRNG algorithm, while in our proposed approach, we would be improving upon the chosen PRNG--giving us the ability to track the cryptographic rigidity between the original PRNG alone and our modified PRNG (which is essentially enhanced through data from a prediction attack).\n",
    "\n",
    "![alt text](
       https://github.com/CSCI4850/S20-team7-project/blob/master/ModdedGAN.png
      )\n",
    "\n",
    "We hope to do this process above with several PRNGs, starting with one of the oldest in 1946 and gradually making our way to newer methods of PRN generation.\n",
    "\n",
    ">[Middle-square method (1946)](https://en.wikipedia.org/wiki/Middle-square_method)<br/>\n",
    "[Lehmer generator (1951)](https://en.wikipedia.org/wiki/Lehmer_random_number_generator)<br/>\n",
    "[Linear congruential generator (1958)](https://en.wikipedia.org/wiki/Linear_congruential_generator)<br/>\n",
    "[Lagged Fibonacci (1965)](https://en.wikipedia.org/wiki/Lagged_Fibonacci_generator)<br/>\n",
    "[Wichmann-Hill generator (1982)](https://en.wikipedia.org/wiki/Wichmann%E2%80%93Hill)<br/>\n",
    "[Park-Miller generator (1988)](https://en.wikipedia.org/wiki/Lehmer_random_number_generator)<br/>\n",
    "[Maximally periodic reciprocals (1992)](https://en.wikipedia.org/wiki/Sophie_Germain_prime)<br/>\n",
    "[Mersenne Twister (1998)](https://en.wikipedia.org/wiki/Mersenne_Twister)<br/>\n",
    "\n",
    "More succinctly put, we will train the generative network with outputs of the chosen PRNG method, so once our generative network is sufficiently modeling the PRNG, it can appropriately improve the method of PRN generation based on predictions from the predictive network.\n",
    "\n",
    "Theoretically, we will be able to look at the data and stochastically rank each PRNG on how much the generative network had to change the weights to be more effective, this should also tell us how \"good\" each PRNG is at generating \"random\" numbers.\n",
    "The only stipulation on the latter is that the weights in the generative network should only change if the predictive network successfully predicts the generated value.\n",
    "\n",
    "#### *Our hypothesis comes in two parts:*\n",
    "1) We predict there will be a positive trend over time on the cryptographic strength of each subsequent PRNG, given the nature of the increasing importance of stronger PRNGs.\n",
    "\n",
    "2) We also predict that as we get into cryptographically stronger generation methods, our prediction success rates (even with learning) will be less effective.\n",
    "\n",
    "\n",
    "### Practical Application\n",
    "\n",
    "A GAN method similar to this could be used in the future during a real-time prediction attack scenario. If attackers are somehow able to break a strong PRNG that is used to generate numbers for cryptographic methods, a method like this could be used as a defense mechanism to prevent (or slow down) further PRN generation cycles from being compromised.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
