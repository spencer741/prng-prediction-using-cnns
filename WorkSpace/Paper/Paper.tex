% PACKAGES INCLUDED HERE 
% DO NOT NEED TO CHANGE
\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{listings}
\usepackage{csquotes}
\usepackage{float}
\usepackage[caption=false]{subfig}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

% TITLE GOES HERE

\title{Predicting Pseudo Random Values Using Convolutional Neural Networks\\}


% AUTHOR NAMES GOES HERE


\author{
\IEEEauthorblockN{1\textsuperscript{st} Spencer Arnold}
\IEEEauthorblockA{\textit{Department of Computer Science}\\
\textit{Middle Tennessee State University}\\
Murfreesboro, Tennessee \\
email address}\\[0.4cm]  %<------- Extra vertical space
\IEEEauthorblockN{4\textsuperscript{th} Matthew Hawks}
\IEEEauthorblockA{\textit{Department of Computer Science}\\
\textit{Middle Tennessee State University}\\
Murfreesboro, Tennessee \\
email address}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Najib Ali}
\IEEEauthorblockA{\textit{Department of Computer Science}\\
\textit{Middle Tennessee State University}\\
Murfreesboro, Tennessee \\
email address}\\[0.4cm]  %<------- Extra vertical space
\IEEEauthorblockN{5\textsuperscript{th} Ryan Hines}
\IEEEauthorblockA{\textit{Department of Computer Science}\\
\textit{Middle Tennessee State University}\\
Murfreesboro, Tennessee \\
email address}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Jacob Anderson}
\IEEEauthorblockA{\textit{Department of Computer Science}\\
\textit{Middle Tennessee State University}\\
Murfreesboro, Tennessee \\
email address}\\[0.4cm]  %<------- Extra vertical space
\IEEEauthorblockN{6\textsuperscript{th} Tae Kweon}
\IEEEauthorblockA{\textit{Department of Computer Science}\\
\textit{Middle Tennessee State University}\\
Murfreesboro, Tennessee \\
email address}
}

\maketitle

% ABSTRACT 

\begin{abstract}
Sample Abstract:
OBJECTIVE: The role of antibiotic therapy in managing acute bacterial sinusitis (ABS) in children is controversial. The purpose of this study was to determine the effectiveness of high-dose amoxicillin/potassium clavulanate in the treatment of children diagnosed with ABS.

METHODS: This was a randomized, double-blind, placebo-controlled study. Children 1 to 10 years of age with a clinical presentation compatible with ABS were eligible for participation. Patients were stratified according to age (<6 or ≥6 years) and clinical severity and randomly assigned to receive either amoxicillin (90 mg/kg) with potassium clavulanate (6.4 mg/kg) or placebo. A symptom survey was performed on days 0, 1, 2, 3, 5, 7, 10, 20, and 30. Patients were examined on day 14. Children’s conditions were rated as cured, improved, or failed according to scoring rules.

RESULTS: Two thousand one hundred thirty-five children with respiratory complaints were screened for enrollment; 139 (6.5%) had ABS. Fifty-eight patients were enrolled, and 56 were randomly assigned. The mean age was 6630 months. Fifty (89%) patients presented with persistent symptoms, and 6 (11%) presented with nonpersistent symptoms. In 24 (43%) children, the illness was classified as mild, whereas in the remaining 32 (57%) children it was severe. Of the 28 children who received the antibiotic, 14 (50%) were cured, 4 (14%) were improved, 4(14%) experienced treatment failure, and 6 (21%) withdrew. Of the 28children who received placebo, 4 (14%) were cured, 5 (18%) improved, and 19 (68%) experienced treatment failure. Children receiving the antibiotic were more likely to be cured (50% vs 14%) and less likely to have treatment failure (14% vs 68%) than children receiving the placebo.

CONCLUSIONS: ABS is a common complication of viral upper respiratory infections. Amoxicillin/potassium clavulanate results in significantly more cures and fewer failures than placebo, according to parental report of time to resolution.” (9)

\end{abstract}


% KEYWORDS

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

% INTRODUCTION SECTION
\section{Introduction}
Observation:

Society relies on Pseudo-random numbers for a multitude of reasons. Whether the usecases fall in Information Security or, more broadly, modeling and simulation, we think it is important to not only push the boundary for the modern pseudo random number directly, but to also analyze the history of pseudo randomness to aide in that effort.

Whether true randomness is a inhibition of the human perception or not, there is clear need to push the modern pseudo-random number closer to converging on perceived "true" randomness.

Questions/Aim:

To train a neural net to predict PRNs from a chosen PRNGs output. We aim to train neural nets to predict values of assigned PRNGs and track attributes of each to make generalized conclusions about the development of PRNGs with respect to time and any PRN correlations we uncover.

Hypotheses:

1) We predict there will be a positive trend over time on the cryptographic strength of each subsequent PRNG, given the nature of the increasing importance of stronger PRNGs.

2) We also predict that as we get into cryptographically stronger generation methods, our prediction success rates (even with learning) will be less effective.

3) We expect to uncover correlations in Pseudo-random numbers based on each individual generator, and aim to extract more generalized correlations between generators themselves.

% BACKGROUND SECTION
\section{Background}


% METHODS SECTION
\section{Methods}
\subsection{Seeding Method}
We went with a seed generation method that allowed a way to introduce some minor level of entropy to avoid letting the neural network aimlessly swim through the entropy of a strong seed instead of gaining stochastic insight on the data from the PRNG.

The seed generation method we chose derives from the concept of using the system time as an element for seed generation. The specific implementation we chose took inspiration from Microsoft's .NET system.datetime.ticks property. \cite{msoftdocs} We chose to single out this method due to its documentation and unique simplicity. System time is widely used as a parameter for modern seed generation methods. 

To put it more trivially: "a pseudo-random number generator is a deterministic algorithm that, given an initial number (called a seed), generates a sequence of numbers that adequately satisfy statistical randomness tests. Since the algorithm is deterministic, the algorithm will always generate the exact same sequence of numbers if it's initialized with the same seed. That's why system time (something that changes all the time) is usually used as the seed for random number generators." \cite{pseudoquote}

According to the Microsoft documentation, "A single tick represents one hundred nanoseconds or one ten-millionth of a second. There are 10,000 ticks in a millisecond, or 10 million ticks in a second. The value of this property represents the number of 100-nanosecond intervals that have elapsed since 12:00:00 midnight, January 1, 0001 in the Gregorian calendar." \cite{msoftdocs}

We used a fairly similar Python port, as seen in Figure ~\ref{fig:Default Seeding Method}

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{./Images/Ticks.png}
\caption{Default Seeding Method}
\label{fig:Default Seeding Method}
\end{figure}

This python port of Microsoft's tick method can be attributed to "mhawke" on StackOverflow. \cite{tickport} The author had some noteworthy comments about this implementation, namely some porting side effects:
\begin{enumerate}
    \item UTC times are assumed.
    \item The resolution of the DateTime object is given by DateTime.resolution, which is DateTime.timedelta(0, 0, 1) or microsecond resolution (1e-06 seconds). CSharp Ticks are purported to be 1e-07 seconds.
\end{enumerate}

For experimental needs, we made additional changes to the implementation:
\begin{enumerate}
    \item Changing start time from January 1, 0001 to January 1, 1970, which effectively reduced the length of the seed for experimental purposes.
    \item Slicing the last 6 digits of the ticks result to acquire more digit variation for frequent invocation.
\end{enumerate}

The final modified method allows enough spread between frequently retrieved ticks, where we are assuming reasonable pseudo-unpredictability. This serves as a simplistic but constantly changing control mechanism for being able to seed PRNGs and test experimental outcomes. While not the most cryptographically strong, we needed a way to have some controlled aspect of seed generation to feed into generators of varying cryptographic complexity (to have some baseline of comparison).

The original idea was to feed each PRNG different seeds from the same seed generator; however, many PRNG algorithms impose strict seed requirements to pass tests of randomness. Out of the five PRNG methods we implemented, Lagged Fibonacci was the only one that had special seed requirements, so we created a separate seed generator based on the same fundamental ticks generation method, but modified to meet the restrictions. Other PRNGs not implemented in this research that impose seed restrictions include Wichmann-Hill (which accepts three different seeds) and Maximally Periodic Reciprocals (which requires a Sophie Prime), among others.

You might ask: won't different seed generators introduce flaws or bias in the experiment? Well, it depends on what you are testing. In our case, we are strictly testing the "complexity" of the generator itself, so supplying a seed that is not blatantly predictable but also not unpredictable was sufficient. Our goal was to allow the characteristics of the generator to be exposed, for we were cracking the "complexity" of the generation algorithm, not the complexity of an arbitrary seed.

\subsection{PRNG Implementations}
For the experiment, we chose five PRNG methods by year of invention. We attempted to choose PRNG methods that displayed popularity in the world of pseudo random number generation while making sure the year of invention was reasonably distributed among the group. Choosing five PRNGs allowed us to focus on implementations, while leaving future research opportunities open for working with other PRNGs that we did not cover.

Chosen PRNGs:
\begin{itemize}
  \item Middle-square method (1946)
  \item Linear congruential generator (1958)
  \item Lagged Fibonacci (1965)
  \item Park-Miller (1988)
  \item Mersenne Twister (1998)
\end{itemize}

The call definition of any given PRNG function is as follows:
\begin{lstlisting}
PRNGfunc(seed, n) 
\end{lstlisting}
This is so we can experimentally automate the calling of each PRNG without getting too complex.
The given PRNG function should return a list of n generated numbers using the generation method.
All other parameters outside of the seed and n are default values.

For example, if the call was PRNG(seed, 10),
it might return something like
[3,5,10,1,31,17,2,4,6,7]

We control parsing the n-length list and handling seeds externally. 
This plays logically with separation of concerns for our usecase.

While python generators can be useful iterating over previously generated iterables,
we did not want to clutter our experimental execution code, so we stuck to a classical
internal handling of all iterations.

Below are short descriptions and non-inclusive general notes of each PRNG and any implementation notes made during the developement process.


\noindent\textbf{Middle Square}:
\begin{itemize}
    \item "To generate a sequence of n-digit pseudorandom numbers, an n-digit starting value is created and squared, producing a 2n-digit number. If the result has fewer than 2n digits, leading zeroes are added to compensate. The middle n digits of the result would be the next number in the sequence, and returned as the result. This process is then repeated to generate more numbers." \cite{MiddleSquare}
\end{itemize}
Notes:
\begin{itemize}
    \item Generally the value of the seed has to be even, but can be padded with leading zeros.
    \item If the middle n digits are all zeroes, the generator then outputs zeroes indefinitely. If the first half of a number in the sequence is zeroes, the subsequent numbers eventually converges to zero.
\end{itemize}


\noindent\textbf{Linear Congruential}:
\begin{itemize}
    \item A linear congruential generator is a PRNG that represents an "additive  congruential  method", with foundations in improving upon "unsatisfactory" tests with entropy in fibonacci sequences. \cite{10.1145/321008.321019}
\end{itemize}
Notes:
\begin{itemize}
    \item The generator is not sensitive to the choice of c, 
as long as it is relatively prime to the modulus 
(e.g. if m is a power of 2, then c must be odd), 
so the value c=1 is commonly chosen.
    \item If c = 0, the generator is often called a multiplicative
congruential generator (MCG), or Lehmer RNG (which is used for our implementation). If c ≠ 0, the
method is called a mixed congruential generator.
    \item Parameters were chosen based on $2^32$ numbers in table 2 of the article "Tables of linear congruential generators of different sizes and good lattice structure." \cite{LEcuyer1999TablesOL}
\end{itemize}


\noindent\textbf{Lagged Fibonacci}:
\begin{itemize}
    \item The Lagged Fibonacci PRNG is a generalization of the Fibonacci Sequence \cite{lucas1891calcul}, where the sequence is generated based off a seed and the sum of the last two values is the PRN (which is also serves as the next seed).
\end{itemize}
Notes:
\begin{itemize}
    \item It is refered to a "lagged" generator, because "j" and "k" lag behind the generated pseudorandom value. 
    \item Our implementation is called a "two-tap" generator, in that you are using 2 values in the sequence 
to generate the pseudorandom number. However, note that a two-tap generator has some problems with 
randomness tests, such as the Birthday Spacings. Creating a "three-tap" generator could
addresses this problem.
\end{itemize}


\noindent\textbf{Park Miller}:
\begin{itemize}
    \item Park Miller (also known as Lehmer) can be viewed as a particular case of the Lemher PRNG, which is a particular case of the linear congruential PRNG, where c=0 and particular parameters are specified.
\end{itemize}
Notes:
\begin{itemize}
    \item "In 1988, Park and Miller \cite{10.1145/63039.63042}, suggested a Lehmer RNG with particular parameters m = 231 − 1 = 2,147,483,647 (a Mersenne prime M31) and a = 75 = 16,807 (a primitive root modulo M31), now known as MINSTD." \cite{ParkMiller}
\end{itemize}


\noindent\textbf{Mersenne Twister}:
\begin{itemize}
    \item "The Mersenne Twister algorithm is based on a matrix linear recurrence over a finite binary field" \cite{10.1145/146382.146383}
\end{itemize}
Notes:
\begin{itemize}
    \item This PRNG is similar to a common LFSR. Its MT19937 implementation is probably the most commonly used modern PRNG. 
    \item It is also the default generator in the Python language starting from version 2.3. 
    \item For our implementation, we used numpy's version of the Mersenne Twister.
\end{itemize}


\subsection{Experimental Setup}


The following is a vastly informal mathematical representation of our experimental model, using a loose coupling of TLA+ notation and some set-builder theory. Further explanations and graphics will follow to aide in interpretation of the design of the experimental setup.

\begin{displayquote}
Let $n$ be any arbitrary natural number such that $\{n \in \mathbb{N}\}$.
Let $E$ represent the experiment definition.
Let $seed$ be a function such that when called will return a seed.
Given $k$ and $S_n$, let $prng$ be a function such that when called will return a set of pseudo random numbers,
dervied from an initial seed $S_n$ and a given algorithm, where the length of the set is $k$.
\end{displayquote}

\begin{figure}[H]
\centering
\includegraphics[width=.5\linewidth]{./Images/ModelNotated.png}
\caption{Notated Experiment Model}
\label{fig:Notated Experiment Model}
\end{figure}


Given an enumerated set, $\mathbb{N}$,
where $prng$ is the chosen PRNG, $S_n$ is $n$th seed in the iteration, and $k$ is the length of the desired output vector ($\mathbb{L}_n$) of $prng$,
$\mathbb{X}_n$ represents an enumerated set containing $n$ sets of $k-1$ values generated from a PRNG ($prng$) and each $\mathbb{X}_n$ set is dervied from a different seed. $\mathbb{Y}_n$ represents a set containing $n$ sets of $k$th values with a direct mapping to each $\mathbb{X}_n$ such that $\mathbb{X}_n \mapsto \mathbb{Y}_n$. $P$ a predictor function that represents a convolutional neural network, which takes in $\mathbb{X}_n$ and $\mathbb{Y}_n$, yields a new set $\mathbb{B}_n$ implied from $\mathbb{X}_n$, where the model trains $\mathbb{B}_n$ to be similar or equal to $\mathbb{Y}_n$ based on back-propagation due to previous predictions, thus using supervised learning to build a regression model.

For a simplified graphical representation of the latter description, please reference the predictive model in Figure~\ref{fig:Predictive Model}, the simplified experimental model in Figure~\ref{fig:Simplified Experimental Model} , and the granular view of the experimental model in Figure~\ref{fig:Granular Experimental Model}.


\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{./Images/SimpleModel.png}
\caption{Simplified Experimental Model}
\label{fig:Simplified Experimental Model}
\end{figure}

Referencing Figure ~\ref{fig:Simplified Experimental Model}, which is a visualization of what a 1-dimensional architecture of our experiment might entail, the predictive neural net as visualized in Figure ~\ref{fig:Predictive Model} is fed outputs of a specific PRNG, which will attempt to predict an kth value, based on previous k-1 values in a particular set of input data, where each set is generated based off of a single unique seed. After being trained on, ideally thousands of sets, the predictive network will form a better stochastic "understanding" of how the PRNG works underneath, thus being able to more accurately predict numbers generated from that PRNG in the future (i.e.,a supervised regression model). The backpropagation will flow through the predictive network to acheive the adversarial nature of a traditional GAN setup, but in reality, we won't be using a generative network, but rather a PRNG algorithm, so the generative part of our setup won't be defensive.


\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{./Images/PredictiveModel.png}
\caption{Predictive Model}
\label{fig:Predictive Model}
\end{figure}

Referencing Figure ~\ref{fig:Predictive Model}, which is a visualization of the layer stack we used for the predictive network in each trial of the experiment, it "consists of four stacked convolutional layers, each with 4 filters, kernel size 2, and stride 1, followed by a max pooling layer and two FCFF layers with 4 and 1 units, respectively. The stack of convolutional layers allow the network to discover complex patterns in the input." \cite{debernardi2018pseudo} Explain here how/why we used that research's dicriminator model in our experiment exactly, except we implemented Relus instead of leaky-relus. Talk about the input xdata and ydata shape and reference the granular model Figure ~\ref{fig:Granular Experimental Model} for detail on how these data are aggregated.



\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{./Images/GranularModel.png}
\caption{Granular Experimental Model}
\label{fig:Granular Experimental Model}
\end{figure}

Talk generally about the data aggregation process, just walk through it.

\subsection{Experimental Execution}
Highlight configuration parameters used to execute the experiment.
Make a note the reasoning behind our small num of epochs and general reasoning behind all parameters. 


% RESULTS SECTION
\section{Results}

\begin{figure}[H]
\centering
\subfloat[Subfigure 1 list of figures text][Testing Regression Plot]{
\includegraphics[width=0.4\linewidth]{./Images/Middle_Square_Reg.png}
\label{fig:MS_A}}
\qquad
\subfloat[Subfigure 2 list of figures text][Training Loss Plot]{
\includegraphics[width=0.4\linewidth]{./Images/Middle_Square_Loss.png}
\label{fig:MS_B}}
\caption{Middle Square Results}
\label{fig:MS}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Subfigure 1 list of figures text][Testing Regression Plot]{
\includegraphics[width=0.4\linewidth]{./Images/Linear_Cong_Reg.png}
\label{fig:LCG_A}}
\qquad
\subfloat[Subfigure 2 list of figures text][Training Loss Plot]{
\includegraphics[width=0.4\linewidth]{./Images/Linear_Cong_Loss.png}
\label{fig:LCG_B}}
\caption{Linear Congruential Results}
\label{fig:LCG}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Subfigure 1 list of figures text][Testing Regression Plot]{
\includegraphics[width=0.4\linewidth]{./Images/Lagged_Fib_Reg.png}
\label{fig:LF_A}}
\qquad
\subfloat[Subfigure 2 list of figures text][Training Loss Plot]{
\includegraphics[width=0.4\linewidth]{./Images/Lagged_Fib_Loss.png}
\label{fig:LF_B}}
\caption{Lagged Fibonacci Results}
\label{fig:LF}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Subfigure 1 list of figures text][Testing Regression Plot]{
\includegraphics[width=0.4\linewidth]{./Images/Park_Miller_Reg.png}
\label{fig:PM_A}}
\qquad
\subfloat[Subfigure 2 list of figures text][Training Loss Plot]{
\includegraphics[width=0.4\linewidth]{./Images/Park_Miller_Loss.png}
\label{fig:PM_B}}
\caption{Park Miller Results}
\label{fig:PM}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Subfigure 1 list of figures text][Testing Regression Plot]{
\includegraphics[width=0.4\linewidth]{./Images/Twister_Reg.png}
\label{fig:MT_A}}
\qquad
\subfloat[Subfigure 2 list of figures text][Training Loss Plot]{
\includegraphics[width=0.4\linewidth]{./Images/Twister_Loss.png}
\label{fig:MT_B}}
\caption{Mersenne Twister Results}
\label{fig:MT}
\end{figure}

% DISCUSSION SECTION
\section{Discussion}

Below are general notes for the discussion section....

Mistakes Made / how the research could be made more robust.

uncertainties

Practical Application:
A GAN method similar to this could be used in the future during a real-time prediction attack scenario. If attackers are somehow able to break a strong PRNG that is used to generate numbers for cryptographic methods, a method like this could be used as a defense mechanism to prevent (or slow down) further PRN generation cycles from being compromised.

This provides a better framework for implementing new PRNGs, seed methods, and experimental designs... and fixing ours to be more mathematically accurate or more robust.

With the results we obtained by doing mere 20 epochs, a lot heavier training and testing could be implemented with powerful computation power.

The base code, such as seeding and PRNG code can also be significantly improved upon to be more mathematically accurate and produce better training and testing of models...


.. code:: ipython3

    % REFERENCES
    % THIS IS CREATED AUTOMATICALLY
    \bibliographystyle{IEEEtran}
    \bibliography{References} % change if another name is used for References file

\end{document}
